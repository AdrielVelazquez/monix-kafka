/*
 * Copyright (c) 2014-2016 by its authors. Some rights reserved.
 * See the project homepage at: https://github.com/monixio/monix-kafka
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package monix.kafka

import java.io.File
import java.util.Properties
import scala.concurrent.duration._

import com.typesafe.config.{Config, ConfigFactory}

import monix.kafka.config._

/** Configuration for Kafka Consumer.
  *
  * For the official documentation on the available configuration
  * options, see
  * [[https://kafka.apache.org/082/documentation.html#consumerconfigs Consumer Configs]]
  * on `kafka.apache.org`.
  *
  * @param groupId is the `group.id` setting, a unique string
  *        that identifies the consumer group this consumer
  *        belongs to.
  *
  * @param zookeeperConnect is the `zookeeper.connect` setting,
  *        a list of host/port pairs to use for establishing
  *        the initial connection to the Zookeeper cluster.
  *
  * @param consumerId is the `consumer.id` setting, a unique string
  *        that identifies the consumer (will be autogenerated if not set).
  *
  * @param socketTimeout is the `socket.timeout.ms` setting,
  *        the socket timeout for network requests.
  *
  * @param socketReceiveBufferInBytes is the `socket.receive.buffer.bytes`
  *        setting, the size of the socket receive buffer for network requests.
  *
  * @param fetchMessageMaxBytes is the `fetch.message.max.bytes`
  *        setting, the maximum amount of data per-partition the
  *        server will return.
  *
  * @param numConsumerFetchers is the `num.consumer.fetchers` setting,
  *        the number of fetcher threads to spawn.
  *
  * @param autoCommitEnable is the `auto.commit.enable` setting.
  *        If true the consumer's offset will be periodically committed
  *        in the background.
  *
  * @param autoCommitInterval is the `auto.commit.interval.ms` setting,
  *        the frequency of autocommits.
  *
  * @param queuedMaxMessageChunks is the `queued.max.message.chunks`
  *        setting, the maximum number of message chunks that consumer
  *        may buffer.
  *
  * @param rebalanceMaxRetries is the `rebalance.max.retries` setting,
  *        the number of attempts to rebalance the consumer group when a new
  *        consumer joins.
  *
  * @param fetchMinBytes is the `fetch.min.bytes` setting,
  *        the minimum amount of data the server should return
  *        for a fetch request.
  *
  * @param fetchWaitMaxTime is the `fetch.wait.max.ms` setting,
  *        the maximum amount of time the server will block before
  *        answering the fetch request if there isn't sufficient data to
  *        immediately satisfy the requirement given by fetch.min.bytes.
  *
  * @param rebalanceBackoffTime is the `rebalance.backoff.m` setting.
  *        The amount of time to wait before attempting to rebalance the
  *        consumer group.
  *
  * @param refreshLeaderBackoffTime is the `refresh.leader.backoff.ms`
  *        setting. The amount of time to wait before trying to elect
  *        a new leader for a consumer group that has lost one.
  *
  * @param autoOffsetReset is the `auto.offset.reset` setting,
  *        specifying what to do when there is no initial offset in
  *        Kafka or if the current offset does not exist any more
  *        on the server (e.g. because that data has been deleted).
  *
  * @param consumerTimeout is the `consumer.timeout.ms` setting,
  *        which specifies the amount of time to wait before throwing
  *        an exception when there's nothing to consume.
  *
  * @param excludeInternalTopics is the `exclude.internal.topics` setting.
  *        Whether records from internal topics (such as offsets) should be
  *        exposed to the consumer. If set to true the only way to receive
  *        records from an internal topic is subscribing to it.
  *
  * @param partitionAssignmentStrategy is the `partition.assignment.strategy`
  *        setting, which chooses how partitions will be assigned to consumer
  *        streams (`range` or `roundrobin`). Note that `roundrobin` strategy
  *        results in a more even load distribution, but will not work when
  *        consuming from multiple topics.
  *
  * @param clientId is the `client.id` setting,
  *        an id string to pass to the server when making requests.
  *        The purpose of this is to be able to track the source of
  *        requests beyond just ip/port by allowing a logical application
  *        name to be included in server-side request logging.
  *
  * @param zookeeperSessionTimeout is the `zookeeper.session.timeout.ms`
  *        setting, the maximum amount of time to wait for a heartbeat before
  *        initiating a rebalance.
  *
  * @param zookeeperConnectionTimeout is the `zookeeper.connection.timeout.ms`
  *        setting, the maximum amount of time the client will wait to
  *        establish a connection to ZooKeeper.
  *
  * @param zookeeperSyncTime is the `zookeeper.sync.time.ms` setting,
  *        the maximum lag allowed for ZK followers.
  *
  * @param offsetsStorage is the `offsets.storage` setting, that controls
  *        where offsets are stored (`zookeeper` or `kafka`).
  *
  * @param offsetsChannelBackoffTime is the `offsets.channel.backoff.ms`
  *        setting, the backoff period when reconnecting the offsets channel
  *        or retrying failed offset fetch/commit requests.
  *
  * @param offsetsChannelSocketTimeout is the `offsets.channel.socket.timeout.ms`
  *        setting. Socket timeout when reading responses for offset fetch/commit
  *        requests.
  *
  * @param offsetsCommitMaxRetries is the `offsets.commit.max.retries` setting,
  *        The maximum amount of retries for commiting the offset. This retry
  *        count only applies to offset commits during shut-down. It does not
  *        apply to commits originating from the auto-commit thread.
  *
  * @param dualCommitEnabled is the `dual.commit.enabled` setting, which
  *        can be used to dual commit offsets to ZooKeeper if using
  *        `kafka` as `offsets.storage`. This is required during migration
  *        from ZooKeeper-based offset storage to Kafka-based offset storage.
  *
  */
final case class KafkaConsumerConfig(
  groupId: String,
  zookeeperConnect: String,
  consumerId: String,
  socketTimeout: FiniteDuration,
  socketReceiveBufferInBytes: Int,
  fetchMessageMaxBytes: Int,
  numConsumerFetchers: Int,
  autoCommitEnable: Boolean,
  autoCommitInterval: FiniteDuration,
  queuedMaxMessageChunks: Int,
  rebalanceMaxRetries: Int,
  fetchMinBytes: Int,
  fetchWaitMaxTime: FiniteDuration,
  rebalanceBackoffTime: FiniteDuration,
  refreshLeaderBackoffTime: FiniteDuration,
  autoOffsetReset: AutoOffsetReset,
  consumerTimeout: FiniteDuration,
  excludeInternalTopics: Boolean,
  partitionAssignmentStrategy: PartitionAssignmentStrategy,
  clientId: String,
  zookeeperSessionTimeout: FiniteDuration,
  zookeeperConnectionTimeout: FiniteDuration,
  zookeeperSyncTime: FiniteDuration,
  offsetsStorage: OffsetsStorage,
  offsetsChannelBackoffTime: FiniteDuration,
  offsetsChannelSocketTimeout: FiniteDuration,
  offsetsCommitMaxRetries: Int,
  dualCommitEnabled: Boolean) {

  def toMap: Map[String, String] = Map(
    "group.id" -> groupId,
    "zookeeper.connect" -> zookeeperConnect,
    "consumer.id" -> consumerId,
    "socket.timeout.ms" -> socketTimeout.toMillis.toString,
    "socket.receive.buffer.bytes" -> socketReceiveBufferInBytes.toString,
    "fetch.message.max.bytes" -> fetchMessageMaxBytes.toString,
    "num.consumer.fetchers" -> numConsumerFetchers.toString,
    "auto.commit.enable" -> autoCommitEnable.toString,
    "auto.commit.interval.ms" -> autoCommitInterval.toMillis.toString,
    "queued.max.message.chunks" -> queuedMaxMessageChunks.toString,
    "rebalance.max.retries" -> rebalanceMaxRetries.toString,
    "fetch.min.bytes" -> fetchMinBytes.toString,
    "fetch.wait.max.ms" -> fetchWaitMaxTime.toMillis.toString,
    "rebalance.backoff.ms" -> rebalanceBackoffTime.toMillis.toString,
    "refresh.leader.backoff.ms" -> refreshLeaderBackoffTime.toMillis.toString,
    "auto.offset.reset" -> autoOffsetReset.id,
    "consumer.timeout.ms" -> consumerTimeout.toMillis.toString,
    "exclude.internal.topics" -> excludeInternalTopics.toString,
    "partition.assignment.strategy" -> partitionAssignmentStrategy.id,
    "client.id" -> clientId,
    "zookeeper.session.timeout.ms" -> zookeeperSessionTimeout.toMillis.toString,
    "zookeeper.connection.timeout.ms" -> zookeeperConnectionTimeout.toMillis.toString,
    "zookeeper.sync.time.ms" -> zookeeperSyncTime.toMillis.toString,
    "offsets.storage" -> offsetsStorage.id,
    "offsets.channel.backoff.ms" -> offsetsChannelBackoffTime.toMillis.toString,
    "offsets.channel.socket.timeout.ms" -> offsetsChannelSocketTimeout.toMillis.toString,
    "offsets.commit.max.retries" -> offsetsCommitMaxRetries.toString,
    "dual.commit.enabled" -> dualCommitEnabled.toString
  )

  def toProperties: Properties = {
    val props = new Properties()
    for ((k, v) <- toMap; if v != null) props.put(k, v)
    props
  }
}

object KafkaConsumerConfig {
  private val defaultRootPath = "kafka"

  lazy private val defaultConf: Config =
    ConfigFactory.load("monix/kafka/default.conf").getConfig(defaultRootPath)

  /** Returns the default configuration, specified the `monix-kafka` project
    * in `monix/kafka/default.conf`.
    */
  lazy val default: KafkaConsumerConfig =
    apply(defaultConf, includeDefaults = false)

  /** Loads the [[KafkaConsumerConfig]] either from a file path or
    * from a resource, if `config.file` or `config.resource` are
    * defined, or otherwise returns the default config.
    *
    * If you want to specify a `config.file`, you can configure the
    * Java process on execution like so:
    * {{{
    *   java -Dconfig.file=/path/to/application.conf
    * }}}
    *
    * Or if you want to specify a `config.resource` to be loaded
    * from the executable's distributed JAR or classpath:
    * {{{
    *   java -Dconfig.resource=com/company/mySpecial.conf
    * }}}
    *
    * In case neither of these are specified, then the configuration
    * loaded is the default one, from the `monix-kafka` project, specified
    * in `monix/kafka/default.conf`.
    */
  def load(): KafkaConsumerConfig =
    Option(System.getProperty("config.file")).map(f => new File(f)) match {
      case Some(file) if file.exists() =>
        loadFile(file)
      case None =>
        Option(System.getProperty("config.resource")) match {
          case Some(resource) =>
            loadResource(resource)
          case None =>
            default
        }
    }

  /** Loads a [[KafkaConsumerConfig]] from a project resource.
    *
    * @param resourceBaseName is the resource from where to load the config
    * @param rootPath is the config root path (e.g. `kafka`)
    * @param includeDefaults should be `true` in case you want to fallback
    *        to the default values provided by the `monix-kafka` library
    *        in `monix/kafka/default.conf`
    */
  def loadResource(resourceBaseName: String, rootPath: String = defaultRootPath, includeDefaults: Boolean = true): KafkaConsumerConfig =
    apply(ConfigFactory.load(resourceBaseName).getConfig(rootPath), includeDefaults)

  /** Loads a [[KafkaConsumerConfig]] from a specified file.
    *
    * @param file is the configuration path from where to load the config
    * @param rootPath is the config root path (e.g. `kafka`)
    * @param includeDefaults should be `true` in case you want to fallback
    *        to the default values provided by the `monix-kafka` library
    *        in `monix/kafka/default.conf`
    */
  def loadFile(file: File, rootPath: String = defaultRootPath, includeDefaults: Boolean = true): KafkaConsumerConfig =
    apply(ConfigFactory.parseFile(file).resolve().getConfig(rootPath), includeDefaults)

  /** Loads the [[KafkaConsumerConfig]] from a parsed
    * `com.typesafe.config.Config` reference.
    *
    * NOTE that this method doesn't assume any path prefix for loading the
    * configuration settings, so it does NOT assume a root path like `kafka`.
    * In case case you need that, you can always do:
    *
    * {{{
    *   KafkaConsumerConfig(globalConfig.getConfig("kafka"))
    * }}}
    *
    * @param source is the typesafe `Config` object to read from
    * @param includeDefaults should be `true` in case you want to fallback
    *        to the default values provided by the `monix-kafka` library
    *        in `monix/kafka/default.conf`
    */
  def apply(source: Config, includeDefaults: Boolean = true): KafkaConsumerConfig = {
    val config = if (!includeDefaults) source else source.withFallback(defaultConf)
    
    KafkaConsumerConfig(
      groupId = config.getString("group.id"),
      zookeeperConnect = config.getString("zookeeper.connect"),
      consumerId = if (config.getIsNull("consumer.id")) null else config.getString("consumer.id"),
      socketTimeout = config.getInt("socket.timeout.ms").millis,
      socketReceiveBufferInBytes = config.getInt("socket.receive.buffer.bytes"),
      fetchMessageMaxBytes = config.getInt("fetch.message.max.bytes"),
      numConsumerFetchers = config.getInt("num.consumer.fetchers"),
      autoCommitEnable = config.getBoolean("auto.commit.enable"),
      autoCommitInterval = config.getInt("auto.commit.interval.ms").millis,
      queuedMaxMessageChunks = config.getInt("queued.max.message.chunks"),
      rebalanceMaxRetries = config.getInt("rebalance.max.retries"),
      fetchMinBytes = config.getInt("fetch.min.bytes"),
      fetchWaitMaxTime = config.getInt("fetch.wait.max.ms").millis,
      rebalanceBackoffTime = config.getInt("rebalance.backoff.ms").millis,
      refreshLeaderBackoffTime = config.getInt("refresh.leader.backoff.ms").millis,
      autoOffsetReset = AutoOffsetReset(config.getString("auto.offset.reset")),
      consumerTimeout = config.getInt("consumer.timeout.ms").millis,
      excludeInternalTopics = config.getBoolean("exclude.internal.topics"),
      partitionAssignmentStrategy = PartitionAssignmentStrategy(config.getString("partition.assignment.strategy")),
      clientId = config.getString("client.id"),
      zookeeperSessionTimeout = config.getInt("zookeeper.session.timeout.ms").millis,
      zookeeperConnectionTimeout = config.getInt("zookeeper.connection.timeout.ms").millis,
      zookeeperSyncTime = config.getInt("zookeeper.sync.time.ms").millis,
      offsetsStorage = OffsetsStorage(config.getString("offsets.storage")),
      offsetsChannelBackoffTime = config.getInt("offsets.channel.backoff.ms").millis,
      offsetsChannelSocketTimeout = config.getInt("offsets.channel.socket.timeout.ms").millis,
      offsetsCommitMaxRetries = config.getInt("offsets.commit.max.retries"),
      dualCommitEnabled = config.getBoolean("dual.commit.enabled")
    )
  }
}
